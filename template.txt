1. read in data
If url and json:
import json
import urllib2
url = 'https://raw.githubusercontent.com/fzr72725/DAT8/master/data/yelp.json'
data = urllib2.urlopen(url)
records = [json.loads(line) for line in data]

To flatten embedded json:
from pandas.io.json import json_normalize
records_frame = json_normalize(records)

2. check if null
data.isnull().values.any()
data.isnull().sum() # to see how many NaN values

2.5 remove outliers

3. For linear regression:
visualize:
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
<using seabirn>
feature_cols = ['temp', 'season', 'weather', 'humidity']
seaborn.pairplot(bikes, x_vars=feature_cols, y_vars='total', kind='reg')
or, for single variable plotting:
sns.lmplot(x='al', y='ri', data=glass, ci=None)

<using pandas>
fig, axs = plt.subplots(1, len(feature_cols), sharey=True)
for index, feature in enumerate(feature_cols):
    bikes.plot(kind='scatter', x=feature, y='total', ax=axs[index], figsize=(16, 3))
<for non-linear relationship>
# box plot of rentals, grouped by season
bikes.boxplot(column='total', by='season')

<using matplotlib>
plt.scatter(glass.al, glass.ri)
plt.xlabel('al')
plt.ylabel('ri')


Feature explore:
* Check if any highly correlated input variables

For continuous features:
???
# Here you can add or remove features into "feature_cols"
X = bikes[feature_cols]
y = bikes.total
linreg = LinearRegression()
linreg.fit(X, y)
zip(feature_cols, linreg.coef_)


For categorical features:
Ordered categories: transform them to sensible numeric values (example: small=1, medium=2, large=3)
Unordered categories: use dummy encoding (0/1)

# Dummy variables:
season_dummies = pd.get_dummies(bikes.season, prefix='season')
season_dummies.sample(n=5, random_state=1)
# In general, if you have a categorical feature with k possible values, you create k-1 dummy variables.


Feature selection:
from sklearn.cross_validation import train_test_split
# define a function that accepts a list of features and returns testing RMSE
def train_test_rmse(feature_cols):
    X = bikes[feature_cols]
    y = bikes.total
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)
    linreg = LinearRegression()
    linreg.fit(X_train, y_train)
    y_pred = linreg.predict(X_test)
    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))
# compare different sets of features
print train_test_rmse(['temp', 'season', 'weather', 'humidity'])
print train_test_rmse(['temp', 'season', 'weather'])
print train_test_rmse(['temp', 'season', 'humidity'])
* RMSE: The lower the better

# make a Null RMSE score:
y_null = np.zeros_like(y_test, dtype=float)
y_null.fill(y_test.mean()['stars'])
print np.sqrt(metrics.mean_squared_error(y_test, y_null))


Modeling:
-fit
-predict
-evaluate using RMSE
-evaluate using Adjusted R-square

?what is heat map in seaborn?
?confusion matrix
?fill null values